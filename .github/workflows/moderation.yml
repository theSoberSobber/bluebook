name: Moderation for PRs

on:
  pull_request:
    paths:
      - '**/*.json'
    types:
      - opened
      - synchronize

jobs:
  moderation:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout the repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.8'

    - name: Install dependencies
      run: |
        pip install ollama

    - name: Parse the JSON file
      id: parse_json
      run: |
        import json
        import os
        
        file_path = os.getenv('GITHUB_EVENT_PATH')
        with open(file_path, 'r') as f:
            pr_data = json.load(f)
        
        qa_data = pr_data['data']
        questions = list(qa_data.keys())
        answers = list(qa_data.values())
        
        output_data = {
            'questions': questions,
            'answers': answers,
            'pr_data': pr_data
        }
        
        with open('/tmp/qa_data.json', 'w') as outfile:
            json.dump(output_data, outfile)

    - name: Moderation Check with Ollama Model
      id: moderation
      run: |
        import json
        import os
        import ollama
        
        with open('/tmp/qa_data.json', 'r') as infile:
            data = json.load(infile)
        
        qa_data = data['pr_data']['data']
        
        prompt = "You are given a bunch of questions and answers pertaining to a document that contains interview transcripts for candidates that have been selected into companies, please flag the document into suitable for publishing on a public facing site or not. Provide a one liner reason flagging the parts that are problematic. We are defining problematic here as profanities/slurs etc. The text can be flagged if the content doesn't seem relevant to our interview experience publishing site too (basically we are trying to avoid spam and hateful speech). The text can be multilingual. You must output in this format: <Yes/No>. <Reasoning Highlighting the Problematic Parts>.\n\n"
        
        for question, answer in qa_data.items():
            prompt += f"Question: {question}\nAnswer: {', '.join(answer)}\n\n"

        response = ollama.chat(model="mistral", messages=[{"role": "user", "content": prompt}])
        
        moderation_output = response['text']
        
        with open('/tmp/moderation_result.txt', 'w') as result_file:
            result_file.write(moderation_output)
        
        print(moderation_output)

    - name: Upload Moderation Result
      uses: actions/upload-artifact@v3
      with:
        name: moderation_result
        path: /tmp/moderation_result.txt

    - name: Comment moderation result on PR
      uses: peter-evans/comment@v2
      with:
        issue-number: ${{ github.event.pull_request.number }}
        body: |
          Moderation Check Completed. Below are the results:
          ```text
          ${{ steps.moderation.outputs.result }}
          ```