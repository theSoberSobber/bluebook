name: Moderation for PRs

on:
  pull_request:
    paths:
      - '**/*.json'
    types:
      - reopened
      - opened
      - synchronize

jobs:
  moderation:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout the repository
      uses: actions/checkout@v3
      with:
          token: ${{ secrets.PAT }}
          fetch-depth: 0
          ref: ${{ github.event.pull_request.head.ref }}

    - name: Validate Email Domain
      id: email-validation
      run: |
        python responses/scripts/checkEmail.py
        echo "::set-output name=is_valid::$?"
      shell: bash
    
    - name: Close PR if Invalid
      if: steps.email-validation.outputs.is_valid != '0'
      env:
        GH_TOKEN: ${{ secrets.PAT }}
      run: |
        gh pr close ${{ github.event.pull_request.number }} --delete-branch
        exit 0

    # Cache Ollama Installation
    - name: Cache Ollama installation
      id: cache-ollama
      uses: actions/cache@v3
      with:
        path: /usr/local/bin/ollama
        key: ollama-${{ runner.os }}-v1
        restore-keys: |
          ollama-${{ runner.os }}-
      
    - name: Install Ollama (only if not cached)
      if: steps.cache-ollama.outputs.cache-hit != 'true'
      run: |
        curl -fsSL https://ollama.com/install.sh | bash
      shell: bash

    # Cache Mistral Model
    - name: Cache Mistral model
      id: cache-mistral
      uses: actions/cache@v3
      with:
        path: /home/runner/.ollama/models/mistral
        key: mistral-${{ runner.os }}-v1
        restore-keys: |
          mistral-${{ runner.os }}-

    - name: Install the LLM model (mistral) if not cached
      if: steps.cache-mistral.outputs.cache-hit != 'true'
      run: |
        ollama pull mistral
      shell: bash

    - name: Build Prompt
      id: prompt
      run: |
        prompt=$(python responses/scripts/buildPrompt.py)
        echo "Prompt generated: $prompt"
        echo "::set-output name=prompt::$prompt"
      shell: bash

    - name: Run Ollama and get result
      id: ollama
      run: |
        # Get the prompt generated in the Build Prompt step
        prompt="${{ steps.prompt.outputs.prompt }}"
        
        # Call Ollama API with the prompt
        review_result=$(curl -s http://127.0.0.1:11434/api/generate \
          -d '{"model": "mistral", "prompt": "'"$prompt"'", "stream": false}' | jq -r '.response')
        
        # Set the review result as an output for later use
        echo "::set-output name=result::$review_result"
      shell: bash

    - name: Parse Ollama Result
      id: parse-result
      run: |
        parsed_result=$(echo "${{ steps.ollama.outputs.result }}" | python responses/scripts/parseOllamaOutput.py)

        can_automerge=$(echo "$parsed_result" | jq -r '.can_automerge')
        comment=$(echo "$parsed_result" | jq -r '.comment')

        echo $can_automerge
        echo $comment
        
        echo "::set-output name=can_automerge::$can_automerge"
        echo "::set-output name=comment::$comment"
      shell: bash

    - name: Comment
      env:
        GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        gh pr comment --body "${{ steps.parse-result.outputs.comment }}"

    - name: Automerge (if applicable)
      env:
        GH_TOKEN: ${{ secrets.PAT }}
      run: |
        if [[ "${{ steps.parse-result.outputs.can_automerge }}" == "true" ]]; then
          gh pr merge ${{ github.event.pull_request.number }} --rebase --delete-branch
        else
          echo "PR could not be automerged."
        fi
      shell: bash